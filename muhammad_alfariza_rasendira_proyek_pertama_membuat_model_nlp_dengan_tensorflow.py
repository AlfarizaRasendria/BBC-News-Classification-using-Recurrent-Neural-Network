# -*- coding: utf-8 -*-
"""Muhammad Alfariza Rasendira_Proyek Pertama Membuat Model NLP dengan TensorFlow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FiuAqulxs700cRLWuGb4URJ6yi8wKWGQ

# **Submission Proyek Pertama Membuat Model NLP dengan TensorFlow**

**Nama : Muhammad Alfariza Rasendria**

Email : rasendria.alfariza18@gmail.com
"""

import tensorflow as tf
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
import nltk

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/Kaggle'

!kaggle datasets download -d hgultekin/bbcnewsarchive --force

import zipfile

file_path = '/content/bbcnewsarchive.zip'

with zipfile.ZipFile(file_path, 'r') as zip_ref:
    zip_ref.extractall('/tmp/')

base_dir = '/tmp'

df = pd.read_csv(f'{base_dir}/bbc-news-data.csv', sep='\t')
df

df.shape

import matplotlib.pyplot as plt
class_distribution = df['category'].value_counts()
print("Distribusi kelas:")
print(class_distribution)
print(len(class_distribution))

plt.figure(figsize=(10, 6))
plt.bar(class_distribution.index, class_distribution.values, color=['green', 'red','blue'])
plt.xlabel('Kelas')
plt.ylabel('Jumlah Sampel')
plt.title('Distribusi Kelas yang ada pada Dataset')
plt.xticks(class_distribution.index, ['sport', 'business','politics','tech','entertaintment'])
plt.show()

df.isnull().sum()

df = df.drop(['filename','title'],axis=1)
df

category = pd.get_dummies(df.category)
df_processed = pd.concat([df, category], axis=1)
df_processed = df_processed.drop(columns='category')
df_processed

from nltk.corpus import stopwords
import string

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)
stop_words.update(punctuation)

def remove_stopwords_nltk(text):
    words = [word.lower() for word in text.split() if word.lower() not in stop_words]
    return " ".join(words)

df_processed['content'] = df_processed['content'].apply(remove_stopwords_nltk)

def remove_punctuation(text):
    tokenizer = nltk.RegexpTokenizer(r"\w+")
    words = tokenizer.tokenize(text)
    return " ".join(words)

df_processed['content']=df_processed['content'].apply(remove_punctuation)

df_processed['content'] = df_processed['content'].str.lower()

df_processed['content']

sentiment = df_processed['content'].values
label = df_processed[['business', 'entertainment', 'politics','sport','tech']].values

def get_corpus(text):
    words = []
    for i in text:
        for j in str(i).split():
            words.append(j.strip())
    return words

corpus = get_corpus(df_processed.content)
corpus

from collections import Counter
counter = Counter(corpus)
counter

from sklearn.model_selection import train_test_split
trained_sentiment, tested_sentiment, trained_label, tested_label = train_test_split(sentiment, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='<oov>')
tokenizer.fit_on_texts(trained_sentiment)

trained_sequences = tokenizer.texts_to_sequences(trained_sentiment)
tested_sequences = tokenizer.texts_to_sequences(tested_sentiment)

trained_padded = pad_sequences(trained_sequences,padding='post',
maxlen=100,
truncating='post')
tested_padded = pad_sequences(tested_sequences,padding='post',
maxlen=100,
truncating='post')

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length = 100),
    tf.keras.layers.LSTM(32, kernel_regularizer=tf.keras.regularizers.l2(0.01), return_sequences=True),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(32, kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

class custom_Callback_class(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.92 and logs.get('val_accuracy') > 0.92) :
      print("\nProses Training Dihentikan karena Akurasi telah melampaui 92%")
      self.model.stop_training = True

custom_Callback = custom_Callback_class()

num_epochs = 30
history_model = model.fit(trained_padded, trained_label, epochs=num_epochs,
                    validation_data=(tested_padded, tested_label), verbose=2, callbacks = [custom_Callback])

import matplotlib.pyplot as plt
plt.plot(history_model.history['accuracy'], color='green', label='Train Accuracy')
plt.plot(history_model.history['val_accuracy'], color='yellow', label='Validation Accuracy')
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Accuracy', 'Validation Accuracy'], loc='upper left')
plt.show()

plt.plot(history_model.history['loss'], color='red', label='Train Loss')
plt.plot(history_model.history['val_loss'], color='orange', label='Validation Loss')
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train Loss', 'Validation Loss'], loc='upper right')
plt.show()